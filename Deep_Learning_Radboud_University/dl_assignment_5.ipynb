{"cells":[{"cell_type":"markdown","metadata":{"id":"L8OFNTuYnng-"},"source":["# Deep Learning &mdash; Assignment 5"]},{"cell_type":"markdown","metadata":{"id":"LjLfmeY_nnhB"},"source":["Fifth assignment for the 2022 Deep Learning course (NWI-IMC070) of the Radboud University.\n","\n","_Twan van Laarhoven (tvanlaarhoven@cs.ru.nl) and Gijs van Tulder (g.vantulder@cs.ru.nl)_\n","\n","_October 2022_"]},{"cell_type":"markdown","metadata":{"id":"2LVlYia8nnhC"},"source":["-----\n","\n","**Names:** Hauque, Federico - Soler Sanz, Guillem\n","\n","**Group:** 5\n","\n","-----"]},{"cell_type":"markdown","metadata":{"id":"P2Jkh7G0nnhC"},"source":["**Instructions:**\n","* Fill in your names and the name of your group.\n","* Answer the questions and complete the code where necessary.\n","* Keep your answers brief, one or two sentences is usually enough.\n","* Re-run the whole notebook before you submit your work.\n","* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n","* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."]},{"cell_type":"markdown","metadata":{"id":"yo8-0BIUnnhC"},"source":["## Objectives\n","\n","In this assignment you will\n","1. Construct a PyTorch `DataSet`\n","1. Train and modify a transformer network\n","1. Experiment with a translation dataset"]},{"cell_type":"markdown","metadata":{"id":"5_6Sy06fnnhD"},"source":["## Required software\n","\n","If you haven't done so already, you will need to install the following additional libraries:\n","* `torch` for PyTorch,\n","* `d2l`, the library that comes with the [Dive into deep learning](https://d2l.ai) book.  \n","  Note: if you get errors, make sure the right version of the d2l library is installed:\n","  `pip install d2l==1.0.0a1.post0`\n","\n","All libraries can be installed with `pip install`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZV_-hPpennhD","outputId":"ced1a352-b860-4a88-acbd-9a8aa070e12d","executionInfo":{"status":"ok","timestamp":1679587566258,"user_tz":-60,"elapsed":29335,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting d2l==1.0.0a1.post0\n","  Downloading d2l-1.0.0a1.post0-py3-none-any.whl (93 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from d2l==1.0.0a1.post0) (1.4.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from d2l==1.0.0a1.post0) (1.22.4)\n","Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (from d2l==1.0.0a1.post0) (0.25.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from d2l==1.0.0a1.post0) (2.27.1)\n","Collecting jupyter\n","  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from d2l==1.0.0a1.post0) (3.7.1)\n","Collecting matplotlib-inline\n","  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym->d2l==1.0.0a1.post0) (6.1.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym->d2l==1.0.0a1.post0) (0.0.8)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym->d2l==1.0.0a1.post0) (2.2.1)\n","Collecting qtconsole\n","  Downloading qtconsole-5.4.1-py3-none-any.whl (120 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.9/120.9 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.9/dist-packages (from jupyter->d2l==1.0.0a1.post0) (6.1.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.9/dist-packages (from jupyter->d2l==1.0.0a1.post0) (6.5.4)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.9/dist-packages (from jupyter->d2l==1.0.0a1.post0) (7.7.1)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.9/dist-packages (from jupyter->d2l==1.0.0a1.post0) (6.3.0)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.9/dist-packages (from jupyter->d2l==1.0.0a1.post0) (5.3.4)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0a1.post0) (5.12.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0a1.post0) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0a1.post0) (2.8.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0a1.post0) (8.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0a1.post0) (23.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0a1.post0) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0a1.post0) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0a1.post0) (1.4.4)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->d2l==1.0.0a1.post0) (4.39.2)\n","Requirement already satisfied: traitlets in /usr/local/lib/python3.9/dist-packages (from matplotlib-inline->d2l==1.0.0a1.post0) (5.7.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->d2l==1.0.0a1.post0) (2022.7.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->d2l==1.0.0a1.post0) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->d2l==1.0.0a1.post0) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->d2l==1.0.0a1.post0) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->d2l==1.0.0a1.post0) (2.0.12)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0a1.post0) (3.15.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->d2l==1.0.0a1.post0) (1.16.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (6.1.12)\n","Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (7.9.0)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (6.2)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter->d2l==1.0.0a1.post0) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter->d2l==1.0.0a1.post0) (3.6.2)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter->d2l==1.0.0a1.post0) (3.0.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-console->jupyter->d2l==1.0.0a1.post0) (2.0.10)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from jupyter-console->jupyter->d2l==1.0.0a1.post0) (2.6.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (4.11.2)\n","Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (5.3.0)\n","Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (5.8.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (2.1.2)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.4)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.2.2)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (1.5.0)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (1.2.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (6.0.0)\n","Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (3.1.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (4.9.2)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.7.1)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.7.2)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.8.4)\n","Requirement already satisfied: Send2Trash>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (1.8.0)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (0.17.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (21.3.0)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (0.16.0)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (23.2.1)\n","Collecting qtpy>=2.0.1\n","  Downloading QtPy-2.3.0-py3-none-any.whl (83 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0a1.post0) (67.6.0)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0a1.post0) (4.4.2)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0a1.post0) (4.8.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.7.5)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.2.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter->d2l==1.0.0a1.post0) (3.1.1)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat>=5.1->nbconvert->jupyter->d2l==1.0.0a1.post0) (2.16.3)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat>=5.1->nbconvert->jupyter->d2l==1.0.0a1.post0) (4.3.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->d2l==1.0.0a1.post0) (0.2.6)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.9/dist-packages (from terminado>=0.8.3->notebook->jupyter->d2l==1.0.0a1.post0) (0.7.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook->jupyter->d2l==1.0.0a1.post0) (21.2.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert->jupyter->d2l==1.0.0a1.post0) (2.4)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert->jupyter->d2l==1.0.0a1.post0) (0.5.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.8.3)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->d2l==1.0.0a1.post0) (0.19.3)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->d2l==1.0.0a1.post0) (22.2.0)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->d2l==1.0.0a1.post0) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->d2l==1.0.0a1.post0) (2.21)\n","Installing collected packages: qtpy, matplotlib-inline, jedi, qtconsole, jupyter, d2l\n","Successfully installed d2l-1.0.0a1.post0 jedi-0.18.2 jupyter-1.0.0 matplotlib-inline-0.1.6 qtconsole-5.4.1 qtpy-2.3.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n"]}],"source":["!pip install torch\n","!pip install d2l==1.0.0a1.post0\n","%matplotlib inline\n","from d2l import torch as d2l\n","import math\n","from random import Random\n","from typing import List\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch.utils.data import (IterableDataset, DataLoader)\n","import matplotlib.pyplot as plt\n","\n","device = d2l.try_gpu()"]},{"cell_type":"markdown","metadata":{"id":"rUtGLSN2nnhF"},"source":["## 5.1 Learning to calculate (5 points)\n","\n","In this assignment we are going to train a neural network to do mathematics.\n","When communicating between humans, mathematics is expressed with words and formulas.\n","The simplest of these are formulas with a numeric answer. For example, we might ask what is `100+50`, to which the answer is `150`.\n","\n","To teach a computer how to do this task, we are going to need a dataset.\n","\n","Below is a function that generates a random formula. Study it, and see if you understand its parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0VbXQg3ynnhF"},"outputs":[],"source":["def random_integer(length: int, signed: bool = True, rng: Random = Random()):\n","    max = math.pow(10, length)\n","    min = -max if signed else 0\n","    return rng.randint(min, max)\n","\n","def random_formula(complexity: int, signed: bool = True, rng: Random = Random()):\n","    \"\"\"\n","    Generate a random formula of the form \"a+b\" or \"a-b\".\n","    complexity is the maximum number of digits in the numbers.\n","    \"\"\"\n","    a = random_integer(complexity, signed, rng)\n","    b = random_integer(complexity, False, rng)\n","    is_addition = not signed or rng.choice([False, True])\n","    if is_addition:\n","        return (f\"{a}+{b}\", str(a + b))\n","    else:\n","        return (f\"{a}-{b}\", str(a - b))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZ3M6fRPnnhG","outputId":"d2ce9b29-f0e6-4307-cdc0-15aad544b168","executionInfo":{"status":"ok","timestamp":1679587891813,"user_tz":-60,"elapsed":13,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('838-984', '-146')"]},"metadata":{},"execution_count":6}],"source":["seed = None\n","random_formula(3, rng=Random(seed))"]},{"cell_type":"markdown","metadata":{"id":"qI6tRcQ3nnhG"},"source":["Note that the `rng` argument allows us to reproduce the same random numbers, which you can verify by running the code below multiple times. But if you change the seed to `None` then the random generator is initialized differently each time."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nrKOheFynnhH","outputId":"6ad1e5b2-7c25-43c8-e93f-4f88cafb8071","executionInfo":{"status":"ok","timestamp":1679587911272,"user_tz":-60,"elapsed":17,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["896-302 = 594\n","-713-540 = -1253\n","198-160 = 38\n","-51+12 = -39\n","343-782 = -439\n"]}],"source":["def random_formulas(complexity, signed, count, seed):\n","    \"\"\"\n","    Iterator that yields the given count of random formulas\n","    \"\"\"\n","    rng = Random(seed)\n","    for i in range(count):\n","        yield random_formula(complexity, signed, rng=rng)\n","\n","for q, a in random_formulas(3, True, 5, seed):\n","    print(f'{q} = {a}')"]},{"cell_type":"markdown","metadata":{"id":"5XK4obMUnnhH"},"source":["We are going to treat these expressions as sequences of tokens, where each character is a token. In addition we will need tokens to denote begin-of-sequence and end-of-sequence, as well as padding, for which we will use `'<bos>'`, `'<eos>'`, and `'<pad>'` respectively, as is done in the book.\n","\n","[d2l chapter 9.2](https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html) includes an example of tokenizing a string, and it also defines a `Vocab` class that handles converting the tokens to numbers.\n","\n","For this dataset we know beforehand what the vocabulary will be.\n","\n","### Creating a vocabulary\n","\n","**(a) What are the tokens in this dataset? Complete the code below.<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQ17jTvQnnhH"},"outputs":[],"source":["vocab = d2l.Vocab(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-', '*', '/'], reserved_tokens=['<bos>', '<eos>', '<pad>'])"]},{"cell_type":"markdown","metadata":{"id":"Hyv475eDnnhH"},"source":["We can print the vocabulary to double check that it makes sense:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qh1RtrsNnnhI","outputId":"3f413966-b230-4329-a1d0-9b481b6373ef","executionInfo":{"status":"ok","timestamp":1679587964235,"user_tz":-60,"elapsed":419,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 18\n","Vocabulary: ['*', '+', '-', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<bos>', '<eos>', '<pad>', '<unk>']\n"]}],"source":["print('Vocabulary size:', len(vocab))\n","print('Vocabulary:', vocab.idx_to_token)"]},{"cell_type":"markdown","metadata":{"id":"mTht0wFannhI"},"source":["Note that the d2l Vocab class includes a `'<unk>'` token, for handling unknown tokens in the input."]},{"cell_type":"markdown","metadata":{"id":"S6KXKcUcnnhI"},"source":["We are now ready to tokenize and encode formula.\n","\n","**(b) Complete the code below.<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rq0oOPBqnnhI"},"outputs":[],"source":["def tokenize_and_encode(string: str, vocab=vocab) -> List[int]:\n","  tokens = list(string)\n","  tokens.append('<eos>')\n","  return vocab[tokens[:]]"]},{"cell_type":"markdown","metadata":{"id":"uZHTNmpsnnhI"},"source":["Let's test it on a random formula:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hyhrP1C_nnhJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a87dd79d-9e1c-4022-cb4d-7b0d9209cb02","executionInfo":{"status":"ok","timestamp":1679587996630,"user_tz":-60,"elapsed":472,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The question 943+641 and answer 1584\n","are encoded as [13, 8, 7, 1, 10, 8, 5, 15] and [5, 9, 12, 8, 15]\n"]}],"source":["q, a = random_formula(3, rng=Random(seed))\n","print('The question', q, 'and answer', a)\n","print('are encoded as', tokenize_and_encode(q), 'and', tokenize_and_encode(a))\n","assert ''.join(vocab.to_tokens(tokenize_and_encode(q))) == q + '<eos>'"]},{"cell_type":"markdown","metadata":{"id":"wLxhLp7UnnhJ"},"source":["### Padding and trimming\n","\n","Next, to be able to work with a whole dataset of these encoded sequences, they all need to be the same length.\n","\n","**(c) Implement the function below that pads or trims the encoded token sequence as needed.<span style=\"float:right\"> (1 point)</span>**\n","\n","Hint: see [d2l section 10.5.3](http://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html#loading-sequences-of-fixed-length) for a very similar function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUv5fVPknnhJ"},"outputs":[],"source":["def pad_or_trim(tokens: List[int], target_length: int, vocab=vocab):\n","    if(len(tokens) < target_length):\n","      for _ in range(len(tokens), target_length):\n","        tokens.append(vocab['<pad>'])\n","    elif(len(tokens) > target_length):\n","      tokens = tokens[0:target_length]\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hP-c790-nnhJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4ce3a52d-4fc4-48c2-bee6-89310eb095b5","executionInfo":{"status":"ok","timestamp":1679588208182,"user_tz":-60,"elapsed":302,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[13, 8, 7, 1, 10, 8, 5, 15, 16, 16]"]},"metadata":{},"execution_count":14}],"source":["# pad or trim q to get a sequence of 10 tokens\n","pad_or_trim(tokenize_and_encode(q), 10)"]},{"cell_type":"markdown","metadata":{"id":"-EtSt7ivnnhJ"},"source":["### Translating tokens\n","\n","We can use `vocab.to_tokens` to convert the encoded token sequence back to something more readable:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trDJu100nnhJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"94856d84-784c-4b77-ad99-3c0a9f7fe774","executionInfo":{"status":"ok","timestamp":1679588217032,"user_tz":-60,"elapsed":499,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['9', '4', '3', '+', '6', '4', '1', '<eos>', '<pad>', '<pad>']"]},"metadata":{},"execution_count":15}],"source":["vocab.to_tokens(pad_or_trim(tokenize_and_encode(q), 10))"]},{"cell_type":"markdown","metadata":{"id":"Y0RmCS7KnnhK"},"source":["For convenience, we define the `decode_tokens` function to convert entire lists or tensors:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AKXd8eWAnnhK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8671996c-f489-4262-d1cd-f7a51a86f044","executionInfo":{"status":"ok","timestamp":1679588227925,"user_tz":-60,"elapsed":317,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[['5' '1' '3' '+' '1' '3' '2' '3' '<eos>' '<pad>']\n"," ['4' '1' '2' '+' '4' '2' '<eos>' '<pad>' '<pad>' '<pad>']]\n"]}],"source":["def decode_tokens(t):\n","    # convert a list, tensor, or array of encoded tokens\n","    if isinstance(t, torch.Tensor):\n","        t = t.detach().cpu().numpy()\n","    t = np.asarray(t)\n","    return np.asarray(vocab.to_tokens(list(t.flatten()))).reshape(*t.shape)\n","\n","# convert all tokens at once\n","print(decode_tokens([pad_or_trim(tokenize_and_encode('513+1323'), 10),\n","                     pad_or_trim(tokenize_and_encode('412+42'), 10)]))"]},{"cell_type":"markdown","metadata":{"id":"aicDrNaEnnhK"},"source":["### Creating a dataset\n","\n","The most convenient way to use a data generating function for training a neural network is to wrap it in a PyTorch `Dataset`. In this case, we will use an [IterableDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset), which can be used as an iterator to walk over the samples in the dataset.\n","\n","**(d) Complete the code below.<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cs175Pm9nnhK"},"outputs":[],"source":["class FormulaDataset(IterableDataset):\n","    def __init__(self, complexity, signed, count, seed=None, vocab=vocab):\n","        self.seed = seed\n","        self.complexity = complexity\n","        self.signed = signed\n","        self.count = count\n","        self.vocab = vocab\n","        self.max_question_length = 2 * complexity + 3\n","        self.max_answer_length = complexity + 2\n","\n","    def __iter__(self):\n","      for q, a in random_formulas(self.complexity, self.signed, self.count, self.seed):\n","        tokens_q = tokenize_and_encode(q, self.vocab)\n","        tokens_a = tokenize_and_encode(a, self.vocab)\n","        padded_q = pad_or_trim(tokens_q, self.max_question_length, self.vocab)\n","        padded_a = pad_or_trim(tokens_a, self.max_answer_length, self.vocab)\n","        yield torch.tensor(padded_q), torch.tensor(padded_a)"]},{"cell_type":"markdown","metadata":{"id":"dLOGq48znnhK"},"source":["**(e) Define a training set with 10000 formulas and a validation set with 5000 formulas, both with complexity 3.<span style=\"float:right\"> (1 point)</span>**\n","\n","---\n","\n","\n","\n","Note: make sure that the training and validation set are different."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2KPUzxqCnnhK"},"outputs":[],"source":["complexity = 3\n","signed = True\n","train_data = FormulaDataset(complexity, signed, 10000, 797989)\n","val_data   = FormulaDataset(complexity, signed, 5000, 232653)"]},{"cell_type":"markdown","metadata":{"id":"gngS04XznnhK"},"source":["As usual, we wrap each dataset in a `DataLoader` to create minibatches."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EOWynBeknnhK"},"outputs":[],"source":["# Define data loaders\n","batch_size = 125\n","data_loaders = {\n","    'train': torch.utils.data.DataLoader(train_data, batch_size=batch_size),\n","    'val':   torch.utils.data.DataLoader(val_data, batch_size=batch_size),\n","}\n","\n","# Verify that train and validation are different but deterministic\n","train_loader = data_loaders['train']\n","val_loader = data_loaders['val']\n","assert torch.equal(next(iter(train_loader))[0], next(iter(train_loader))[0])\n","assert torch.equal(next(iter(val_loader))[0], next(iter(val_loader))[0])\n","assert not torch.equal(next(iter(train_loader))[0], next(iter(val_loader))[0])"]},{"cell_type":"markdown","metadata":{"id":"KjhgX6-6nnhL"},"source":["## 5.2 Transformer inputs (10 points)\n","\n","There is a detailed description of the transformer model in [chapter 11 of the d2l book](http://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html). We will not use most the code from the book, and instead use [PyTorch's built-in Transformer layers](https://pytorch.org/docs/stable/nn.html#transformer-layers).\n","\n","However, some details we still need to implement ourselves."]},{"cell_type":"markdown","metadata":{"id":"EkALW7MCnnhL"},"source":["### Masks\n","\n","Training a transformer uses masked self-attention, so we need some masks. Here are two functions that make these masks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHvKlIUnnnhL"},"outputs":[],"source":["def generate_square_subsequent_mask(size, device=device):\n","    \"\"\"\n","    Mask that indicates that tokens at a position are not allowed to attend to\n","    tokens in subsequent positions.\n","    \"\"\"\n","    mask = (torch.tril(torch.ones((size, size), device=device))) == 0\n","    return mask\n","\n","def generate_padding_mask(tokens, padding_token):\n","    \"\"\"\n","    Mask that indicates which tokens should be ignored because they are padding.\n","    \"\"\"\n","    return tokens == torch.tensor(padding_token)"]},{"cell_type":"markdown","metadata":{"id":"UP40M4KYnnhL"},"source":["**(a) Generate a padding mask for a random encoded token string.<span style=\"float:right\"> (1 point)</span>**\n","\n","Hint: make sure that `tokens` is a torch.tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ieA74YlnnhL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a00564ca-6afd-4ace-8e29-e23432658105","executionInfo":{"status":"ok","timestamp":1679588356715,"user_tz":-60,"elapsed":768,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([11,  5,  2,  8, 12, 10, 15, 16, 16])\n","['7' '1' '-' '4' '8' '6' '<eos>' '<pad>' '<pad>']\n","tensor([False, False, False, False, False, False, False,  True,  True])\n"]}],"source":["q, a = random_formula(3, rng=Random(seed))\n","tokens = torch.tensor(pad_or_trim(tokenize_and_encode(q, vocab), 9))\n","padding_mask = generate_padding_mask(tokens, vocab['<pad>'])\n","print(tokens)\n","print(decode_tokens(tokens))\n","print(padding_mask)"]},{"cell_type":"markdown","metadata":{"id":"mnbCHkVunnhL"},"source":["**(b) How will this mask be used by a transformer?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"-CUtaFcrnnhL"},"source":["It will be useful to tell easily which tokens have actual acontent and which are just padding."]},{"cell_type":"markdown","metadata":{"id":"VEmEqsY3nnhL"},"source":["The code below takes the first batch of data from the training set, and it generates a shifted version of the target values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qZalkurbnnhM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4bed3119-356d-4267-8e30-de8cd41a8b97","executionInfo":{"status":"ok","timestamp":1679588376229,"user_tz":-60,"elapsed":15,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[['1' '5' '6' '7' '<eos>']\n"," ['-' '1' '5' '9' '0']\n"," ['4' '7' '6' '<eos>' '<pad>']\n"," ['1' '9' '0' '<eos>' '<pad>']\n"," ['1' '0' '4' '<eos>' '<pad>']]\n","[['<bos>' '1' '5' '6' '7']\n"," ['<bos>' '-' '1' '5' '9']\n"," ['<bos>' '4' '7' '6' '<eos>']\n"," ['<bos>' '1' '9' '0' '<eos>']\n"," ['<bos>' '1' '0' '4' '<eos>']]\n"]}],"source":["x, y = next(iter(train_loader))\n","bos = torch.tensor(vocab['<bos>']).expand(y.shape[0], 1)\n","y_prev = torch.cat((bos, y[:,:-1]), axis=1)\n","\n","\n","\n","# print the first five samples\n","print(decode_tokens(y)[:5])\n","print(decode_tokens(y_prev)[:5])"]},{"cell_type":"markdown","metadata":{"id":"cFVq15gunnhM"},"source":["**(c) Look at the values for the example above. What is `y_prev` used for during training of a transformer model?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"cDGJ8kXrnnhM"},"source":["The y_prev is fed as the target of the transformer model, starting always with the bos token and masked."]},{"cell_type":"markdown","metadata":{"id":"CFLm3frHnnhM"},"source":["**(d) Why do some rows of `y_prev` end in `<eos>`, but not all? Is this a problem?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"j-EVi4PqnnhM"},"source":["Because, in order to have the same size, smaller rows are padded, so their last positions are just concatenations of paddings."]},{"cell_type":"markdown","metadata":{"id":"lDf-b3EJnnhM"},"source":["**(e) Run generate_square_subsequent_mask.<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wsjPyUsYnnhM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3576b02b-cb74-4f6d-bf59-245f9ac97d7b","executionInfo":{"status":"ok","timestamp":1679588421070,"user_tz":-60,"elapsed":338,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True],\n","        [False, False,  True,  True,  True,  True,  True,  True,  True],\n","        [False, False, False,  True,  True,  True,  True,  True,  True],\n","        [False, False, False, False,  True,  True,  True,  True,  True],\n","        [False, False, False, False, False,  True,  True,  True,  True],\n","        [False, False, False, False, False, False,  True,  True,  True],\n","        [False, False, False, False, False, False, False,  True,  True],\n","        [False, False, False, False, False, False, False, False,  True],\n","        [False, False, False, False, False, False, False, False, False]])\n","torch.Size([9, 9])\n"]}],"source":["square_subsequent_mask = generate_square_subsequent_mask(9)\n","\n","print(square_subsequent_mask)\n","print(square_subsequent_mask.shape)"]},{"cell_type":"markdown","metadata":{"id":"1IGHr8H5nnhM"},"source":["**(f) Where should this mask be used? State your answer in terms of `x`,  `y` and `y_prev`.<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"AVKz34AvnnhM"},"source":["This is used to know which tokens are known at each part of the decoding process, so that the algorithm does not know future data. The first element for example, knows nothing about the following ones, hence all the remaining elements in the row are masked with true."]},{"cell_type":"markdown","metadata":{"id":"Ijhvx7hnnnhM"},"source":["**(g) Explain the shape of this mask.<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"MIsI03O8nnhN"},"source":["The mask has the shape of the source on rows and columns."]},{"cell_type":"markdown","metadata":{"id":"KXno-7zGnnhN"},"source":["**(h) When would it make sense to use a different mask?**"]},{"cell_type":"markdown","metadata":{"id":"VwKTYwfHnnhN"},"source":["When the source has a different size."]},{"cell_type":"markdown","metadata":{"id":"prH8bsVnnnhN"},"source":["### Embedding\n","\n","Our discrete vocabulary is not suitable as the input for a transformer. We need an embedding function to map our input vocabulary to a continuous, high-dimensional space.\n","\n","We will use the `torch.nn.Embedding` class to for this. As you can read in the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding), this class maps each token in our vocabulary to a specific point in embedding space, its embedding vector. We will use this embedding vector as the input features for the next layer of our model.\n","\n","The parameters of the embedding are trainable: the embedding vector of each token is optimized along with the rest of the network."]},{"cell_type":"markdown","metadata":{"id":"21IiAhkGnnhN"},"source":["**(i) Define an embedding that maps our vocabulary to a 5-dimensional space.<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUI-uneknnhN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f0468423-fde4-4618-f000-3dc93dfe5090","executionInfo":{"status":"ok","timestamp":1679588506803,"user_tz":-60,"elapsed":491,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding(18, 5)\n"]}],"source":["embedding = torch.nn.Embedding(len(vocab), 5)\n","print(embedding)"]},{"cell_type":"markdown","metadata":{"id":"uwkMbt1InnhN"},"source":["Let's apply the embedding to some sequences from our training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mw9x8BwfnnhN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6db0c57c-62af-4365-e612-b8913cd15130","executionInfo":{"status":"ok","timestamp":1679588554243,"user_tz":-60,"elapsed":341,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[11, 10, 13,  1, 11, 13, 12, 15, 16],\n","        [ 2, 11, 10,  4,  2, 12,  7,  4, 15],\n","        [13,  8,  4,  2,  8, 10,  8, 15, 16]])\n","tensor([[[-1.4076, -0.6411, -0.6227, -1.3610, -0.2963],\n","         [-0.1013,  0.2738,  0.4947, -0.7033, -0.3411],\n","         [ 1.3500, -0.3770, -0.0255,  1.4452, -1.2798],\n","         [-0.3978,  1.9345, -0.3704,  1.2860,  1.4888],\n","         [-1.4076, -0.6411, -0.6227, -1.3610, -0.2963],\n","         [ 1.3500, -0.3770, -0.0255,  1.4452, -1.2798],\n","         [-0.8711, -0.4927, -0.2427, -2.1439, -0.6956],\n","         [ 0.5823, -0.2115, -0.9410,  1.6928, -0.0591],\n","         [-0.1181,  0.3820,  1.5512, -2.2985, -1.4732]],\n","\n","        [[ 0.8064,  0.1181, -1.3883,  0.3764, -1.1776],\n","         [-1.4076, -0.6411, -0.6227, -1.3610, -0.2963],\n","         [-0.1013,  0.2738,  0.4947, -0.7033, -0.3411],\n","         [ 0.7711,  0.3577,  1.0498,  0.5569,  1.9634],\n","         [ 0.8064,  0.1181, -1.3883,  0.3764, -1.1776],\n","         [-0.8711, -0.4927, -0.2427, -2.1439, -0.6956],\n","         [-2.1518,  1.6835,  0.0055, -0.2947, -0.3412],\n","         [ 0.7711,  0.3577,  1.0498,  0.5569,  1.9634],\n","         [ 0.5823, -0.2115, -0.9410,  1.6928, -0.0591]],\n","\n","        [[ 1.3500, -0.3770, -0.0255,  1.4452, -1.2798],\n","         [ 0.9044, -1.3273, -0.6042, -1.2349,  0.1077],\n","         [ 0.7711,  0.3577,  1.0498,  0.5569,  1.9634],\n","         [ 0.8064,  0.1181, -1.3883,  0.3764, -1.1776],\n","         [ 0.9044, -1.3273, -0.6042, -1.2349,  0.1077],\n","         [-0.1013,  0.2738,  0.4947, -0.7033, -0.3411],\n","         [ 0.9044, -1.3273, -0.6042, -1.2349,  0.1077],\n","         [ 0.5823, -0.2115, -0.9410,  1.6928, -0.0591],\n","         [-0.1181,  0.3820,  1.5512, -2.2985, -1.4732]]],\n","       grad_fn=<EmbeddingBackward0>)\n","torch.Size([3, 9])\n","torch.Size([3, 9, 5])\n"]}],"source":["# take the first batch\n","x, y = next(iter(train_loader))\n","# take three samples\n","x = x[:3]\n","# print the shapes\n","print(x)\n","print(embedding(x))\n","print(x.shape)\n","print(embedding(x).shape)"]},{"cell_type":"markdown","metadata":{"id":"UD5BM89nnnhN"},"source":["**(j) Explain the output shape.<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"80OOO8dNnnhO"},"source":["For each input token, the embedding has generated a vector of 5 dimensions that represents is. This was done for each token of each input set (in this case only 1)"]},{"cell_type":"markdown","metadata":{"id":"kSb0U8p5nnhO"},"source":["The size of the embedding vectors, or the dimensionality of the embedding space, does not depend on the number of tokens in our vocabulary. We are free to choose an embedding size that fits our problem.\n","\n","For example, let's try an embedding with 2 dimensions, and plot the initial embedding for the tokens in our vocabulary.\n","\n","**(k) Create an embedding with 2 dimensions and plot the embedding for all tokens.<span style=\"float:right\"> (no points)</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-zvOQK_nnhO","colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"ee699818-e8f3-4d34-a092-a3559971a1fc","executionInfo":{"status":"ok","timestamp":1679588626056,"user_tz":-60,"elapsed":844,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["embedding = torch.nn.Embedding(len(vocab), 5)\n","\n","# embed all tokens of our vocabulary\n","x = torch.arange(len(vocab))\n","emb = embedding(x).detach().cpu().numpy()\n","\n","plt.scatter(emb[:, 0], emb[:, 1]);"]},{"cell_type":"markdown","metadata":{"id":"DuP147RrnnhO"},"source":["As always, we need to balance the complexity of our networks: a larger embedding will increase the number of parameters in our model, but increase the risk of overfitting.\n","\n","**(l) Would this 2-dimensional embedding space be large enough for our problem?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"SDKa2HhYnnhO"},"source":["The space is big enough for defining just a few vectors representing a vocabulary of only 18 tokens. Points can be well distributed among the space."]},{"cell_type":"markdown","metadata":{"id":"B7Z0VEtFnnhO"},"source":["Instead of using an embedding, we could also use a simple one-hot encoding to map the words in the vocabulary to feature vectors. However, practical applications of natural language processing never do this. Why not?\n","\n","**(m) Explain the practical advantage of embeddings over one-hot encoding.<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"s2Laru93nnhO"},"source":["Embeddings require less values to represent a single category in a list. 5 vector dimensions is way easier to manage and operate with than longer vector with only one value non-equal to zero."]},{"cell_type":"markdown","metadata":{"id":"fTUozBfynnhQ"},"source":["## 5.3 `torch.nn.Transformer` (8 points)\n","\n","<div style=\"float: left\"><a href=\"https://cs.ru.nl/~gvtulder/vaswani-fig-1-highlight.png\"><img src=\"https://cs.ru.nl/~gvtulder/vaswani-fig-1-highlight.png\" width=\"300\"></a></div>\n","\n","We now have all required inputs for our transformer.\n","\n","Consult the documentation for the [`torch.nn.Transformer`](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) class of PyTorch. This class implements a full Transformer as described in [\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf), the paper that introduced this architecture.\n","\n","The `Transformer` class implements the main part of the of the Transformer architecture, shown highlighted in the image on the left (see also Fig. 1 in \"Attention Is All You Need\").\n","\n","For a given input sequence, it applies one or more encoder layers, followed by one or more decoder layers, to compute an output sequence that we can then process further.\n","\n","Because the `Transformer` class takes care of most of the complicated parts of the model, we can concentrate providing the inputs and outputs: the grayed-out areas in the image.\n","\n","Check out the parameters for the `Transformer` class and the inputs and outputs of its `forward` function.\n","<br style=\"clear: both\">"]},{"cell_type":"markdown","metadata":{"id":"jevK4ZG-nnhR"},"source":["**(a) Which parameter of the Transformer class should we base on our embedding?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"6-DultHCnnhR"},"source":["The d_model parameter (the first of the transformer constructor function) represents the number of dimensions of the embeddings we are predicting."]},{"cell_type":"markdown","metadata":{"id":"eC5qe3vGnnhR"},"source":["**(b) Given fixed input and output dimensions, which parameters of the Transformer can we use to change the complexity of our network?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"eD99MtwznnhR"},"source":["We could either use more encoding or decoding layers, or even more dimenssions in the embedding."]},{"cell_type":"markdown","metadata":{"id":"UWTuNUcunnhR"},"source":["**(c) Where should we use the masks that we defined earlier?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"_IYwonTnnnhR"},"source":["The masks should be used when feeding the source and target of the Transformer block."]},{"cell_type":"markdown","metadata":{"id":"uvG0BH9wnnhR"},"source":["### Building a network"]},{"cell_type":"markdown","metadata":{"id":"u2KMllCgnnhR"},"source":["**(d) Complete the code for the TransformerNetwork.<span style=\"float:right\"> (5 points)</span>**\n","\n","Construct a network with the following architecture (see the image in the previous section for an overview):\n","1. An embedding layer that embeds the input tokens into a space of size `dim_hidden`.\n","2. A dropout layer (not shown in the image).\n","3. A [Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) with the specified parameters (`dim_hidden`, `num_heads`, `num_layers`, `dim_feedforward`, and `dropout`).<br>Note: you will need to pass `batch_first=True`, to indicate that the first dimension runs over the batch and not over the sequence.\n","4. A final linear prediction layer that takes the output of the transformer to `dim_vocab` possible classes.\n","\n","Don't worry about positional encoding for now, we will add that later.\n","\n","The `forward` function should generate the appropriate masks and combine the layers defined in `__init__` to compute the output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h0egohDhnnhR"},"outputs":[],"source":["class TransformerNetwork(torch.nn.Module):\n","    def __init__(self,\n","                 dim_vocab = len(vocab), padding_token=vocab['<pad>'],\n","                 num_layers=2, num_heads=4, dim_hidden=64, dim_feedforward=64,\n","                 dropout=0.01, positional_encoding=False):\n","        super().__init__()\n","        self.padding_token = padding_token\n","        self.embedding    = torch.nn.Embedding(dim_vocab, dim_hidden)\n","        self.dropout      = torch.nn.Dropout()\n","        self.transformer  = torch.nn.Transformer(dim_hidden, num_heads, num_layers, num_layers, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n","        self.predict      = torch.nn.Linear(dim_hidden, dim_vocab)\n","        if positional_encoding:\n","            self.pos_encoding = ... # Fill this in later\n","        else:\n","            self.pos_encoding = torch.nn.Identity()\n","\n","    def forward(self, src, tgt):\n","        embeded_src = self.dropout(self.embedding(src))\n","        embeded_tgt = self.dropout(self.embedding(tgt))\n","        src_mask = generate_square_subsequent_mask(src.shape[1])\n","        tgt_mask = generate_square_subsequent_mask(tgt.shape[1])\n","        src_padding_mask = generate_padding_mask(src, self.padding_token)\n","        tgt_padding_mask = generate_padding_mask(tgt, self.padding_token)\n","        result = self.transformer(embeded_src, embeded_tgt, src_mask, tgt_mask, src_key_padding_mask=src_padding_mask, tgt_key_padding_mask=tgt_padding_mask)\n","        return self.predict(result)"]},{"cell_type":"markdown","metadata":{"id":"8uIIJ6jqnnhS"},"source":["**(e) Try the transformer with an example batch.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZGxA-g1xnnhS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3121437c-c686-4694-c6a9-e7d487d42027","executionInfo":{"status":"ok","timestamp":1679588801650,"user_tz":-60,"elapsed":15,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["x.shape torch.Size([125, 9])\n","y.shape torch.Size([125, 5])\n","y_prev.shape torch.Size([125, 5])\n","y_pred.shape torch.Size([125, 5, 18])\n"]}],"source":["net = TransformerNetwork()\n","x, y = next(iter(train_loader))\n","bos = torch.tensor(vocab['<bos>']).expand(y.shape[0], 1)\n","y_prev = torch.cat((bos, y[:, :-1]), axis=1)\n","\n","print('x.shape', x.shape)\n","print('y.shape', y.shape)\n","print('y_prev.shape', y_prev.shape)\n","\n","y_pred = net(x, y_prev)\n","print('y_pred.shape', y_pred.shape)\n","\n","# check the shape against what we expected\n","np.testing.assert_equal(list(y_pred.shape), [y.shape[0], y.shape[1], len(vocab)])"]},{"cell_type":"markdown","metadata":{"id":"0-ezLswpnnhS"},"source":["We can convert these predictions to tokens (but they're obviously random):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozKNPcwannhS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"616811ad-fe12-4a03-a68d-1b916fbe7d23","executionInfo":{"status":"ok","timestamp":1679588819215,"user_tz":-60,"elapsed":631,"user":{"displayName":"Guillem Soler","userId":"00913242824559983185"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[['1' '6' '6' '1' '5']\n"," ['/' '*' '6' '5' '6']\n"," ['<pad>' '2' '/' '<pad>' '0']\n"," ['<pad>' '6' '6' '6' '+']\n"," ['<pad>' '<bos>' '6' '6' '6']]\n"]}],"source":["print(decode_tokens(torch.argmax(y_pred, dim=2))[:5])"]},{"cell_type":"markdown","metadata":{"id":"hOkNdaQwnnhS"},"source":["## 5.4 Training (9 points)"]},{"cell_type":"markdown","metadata":{"id":"hD-sd6G4nnhS"},"source":["### Training loop"]},{"cell_type":"markdown","metadata":{"id":"oikUmWYbnnhS"},"source":["**(a) Write a training loop for the transformer model.<span style=\"float:right\"> (4 points)</span>**\n","\n","See last week's assignment for inspiration.\n","The code is mostly the same with the following changes:\n"," * The cross-entropy loss function should ignore all `<pad>` tokens. (Use `ignore_index`, see the [documentation of CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).)\n"," * The accuracy() function from last week should similarly ignore padding tokens (Hint: `correct[y != ignore_index]`)\n"," * The network expects `y_prev` as an extra input.\n"," * The output of the network contains a batch of N samples, with maximum length L, and gives logits over C classes, so it has size (N,L,C). But `CrossEntropyLoss` and `accuracy` expect a tensor of size (N,C,L). You can use [torch.Tensor.transpose](https://pytorch.org/docs/stable/generated/torch.transpose.html) to change the output to the right shape."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G3cY8FqFnnhS"},"outputs":[],"source":["def accuracy(y_hat, y):\n","    y_hat = torch.argmax(y_hat, axis=1).to(y.dtype)\n","    correct = (y_hat == y).to(torch.float32)\n","    return torch.mean(correct[y != vocab['<pad>']])\n","\n","def train(net, data_loaders, epochs=100, lr=0.001, device=device):\n","    \"\"\"\n","    Trains the model net with data from the data_loaders['train'] and data_loaders['val'].\n","    \"\"\"\n","    net = net.to(device)\n","\n","    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","\n","    animator = d2l.Animator(xlabel='epoch',\n","                            legend=['train loss', 'train acc', 'validation acc'],\n","                            figsize=(10, 5))\n","\n","    timer = {'train': d2l.Timer(), 'val': d2l.Timer()}\n","    for epoch in range(0, epochs):\n","      metrics = {'train': d2l.Accumulator(3), 'val': d2l.Accumulator(3)}\n","      for phase in ('train', 'val'):\n","        net.train(phase == 'train')\n","        for i, (x, y) in enumerate(data_loaders[phase]):\n","          timer[phase].start()\n","          bos = torch.tensor(vocab['<bos>']).expand(y.shape[0], 1)\n","          y_prev = torch.cat((bos, y[:, :-1]), axis=1)\n","          y_hat = net(x, y_prev)\n","          transposed = torch.transpose(y_hat, 1, 2)\n","          loss = torch.nn.CrossEntropyLoss(ignore_index = vocab['<pad>'])(transposed, y)\n","          if phase == 'train':\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","          acc = accuracy(transposed, y)\n","          metrics[phase].add(loss * x.shape[0],\n","            acc * x.shape[0],\n","            x.shape[0])\n","          timer[phase].stop()\n","      animator.add(epoch + 1,\n","        (metrics['train'][0] / metrics['train'][2],\n","          metrics['train'][1] / metrics['train'][2],\n","          metrics['val'][1] / metrics['val'][2]))\n","    train_loss = metrics['train'][0] / metrics['train'][2]\n","    train_acc  = metrics['train'][1] / metrics['train'][2]\n","    val_acc    = metrics['val'][1] / metrics['val'][2]\n","    examples_per_sec = metrics['train'][2] * epochs / timer['train'].sum()\n","\n","    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '\n","          f'val acc {val_acc:.3f}')\n","    print(f'{examples_per_sec:.1f} examples/sec '\n","          f'on {str(device)}')"]},{"cell_type":"markdown","metadata":{"id":"KqX3hzcinnhS"},"source":["### Experiment"]},{"cell_type":"markdown","metadata":{"id":"mzNLhAYDnnhT"},"source":["**(b) Train a transformer network. Use 100 epochs with a learning of 0.001<span style=\"float:right\"> (no points)</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSb36eRinnhT","colab":{"base_uri":"https://localhost:8080/","height":479},"outputId":"ebb9f344-2331-4ac8-c4f6-4670e12a329e"},"outputs":[{"output_type":"stream","name":"stdout","text":["train loss 0.813, train acc 0.695, val acc 0.746\n","1724.2 examples/sec on cpu\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 720x360 with 1 Axes>"],"image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"316.55625pt\" version=\"1.1\" viewBox=\"0 0 601.665625 316.55625\" width=\"601.665625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 316.55625 \nL 601.665625 316.55625 \nL 601.665625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 279 \nL 594.465625 279 \nL 594.465625 7.2 \nL 36.465625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p4e7628ff37)\" d=\"M 56.705294 279 \nL 56.705294 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m951db334dd\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.705294\" xlink:href=\"#m951db334dd\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(53.524044 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p4e7628ff37)\" d=\"M 159.184633 279 \nL 159.184633 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"159.184633\" xlink:href=\"#m951db334dd\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(152.822133 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p4e7628ff37)\" d=\"M 261.663972 279 \nL 261.663972 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"261.663972\" xlink:href=\"#m951db334dd\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(255.301472 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p4e7628ff37)\" d=\"M 364.143311 279 \nL 364.143311 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"364.143311\" xlink:href=\"#m951db334dd\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(357.780811 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p4e7628ff37)\" d=\"M 466.62265 279 \nL 466.62265 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"466.62265\" xlink:href=\"#m951db334dd\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(460.26015 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p4e7628ff37)\" d=\"M 569.101989 279 \nL 569.101989 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"569.101989\" xlink:href=\"#m951db334dd\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(559.558239 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     </defs>\n     <g transform=\"translate(300.2375 307.276562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p4e7628ff37)\" d=\"M 36.465625 246.006987 \nL 594.465625 246.006987 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m1a429d1de8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m1a429d1de8\" y=\"246.006987\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.50 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(7.2 249.806206)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p4e7628ff37)\" d=\"M 36.465625 210.895236 \nL 594.465625 210.895236 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m1a429d1de8\" y=\"210.895236\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.75 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(7.2 214.694455)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p4e7628ff37)\" d=\"M 36.465625 175.783485 \nL 594.465625 175.783485 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m1a429d1de8\" y=\"175.783485\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.00 -->\n      <g transform=\"translate(7.2 179.582704)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p4e7628ff37)\" d=\"M 36.465625 140.671734 \nL 594.465625 140.671734 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m1a429d1de8\" y=\"140.671734\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.25 -->\n      <g transform=\"translate(7.2 144.470952)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p4e7628ff37)\" d=\"M 36.465625 105.559982 \nL 594.465625 105.559982 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m1a429d1de8\" y=\"105.559982\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.50 -->\n      <g transform=\"translate(7.2 109.359201)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p4e7628ff37)\" d=\"M 36.465625 70.448231 \nL 594.465625 70.448231 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m1a429d1de8\" y=\"70.448231\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.75 -->\n      <g transform=\"translate(7.2 74.24745)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p4e7628ff37)\" d=\"M 36.465625 35.33648 \nL 594.465625 35.33648 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m1a429d1de8\" y=\"35.33648\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 2.00 -->\n      <g transform=\"translate(7.2 39.135699)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_27\">\n    <path clip-path=\"url(#p4e7628ff37)\" d=\"M 61.829261 19.554545 \nL 66.953228 66.065875 \nL 72.077195 79.424757 \nL 77.201162 83.904906 \nL 82.325129 86.763807 \nL 87.449096 89.545119 \nL 92.573063 92.250749 \nL 97.69703 94.493184 \nL 102.820997 96.643789 \nL 107.944964 99.70908 \nL 113.068931 102.39113 \nL 118.192898 104.944005 \nL 123.316865 107.900813 \nL 128.440832 111.104163 \nL 133.564799 113.586927 \nL 138.688765 115.942415 \nL 143.812732 118.054469 \nL 148.936699 119.775256 \nL 154.060666 121.631837 \nL 159.184633 122.542047 \nL 164.3086 124.156179 \nL 169.432567 125.583341 \nL 174.556534 126.119791 \nL 179.680501 128.076184 \nL 184.804468 129.015423 \nL 189.928435 130.232763 \nL 195.052402 130.495176 \nL 200.176369 132.049303 \nL 205.300336 133.166389 \nL 210.424303 133.247039 \nL 215.54827 133.904213 \nL 220.672237 134.861761 \nL 225.796204 135.46259 \nL 230.92017 135.777711 \nL 236.044137 136.723432 \nL 241.168104 137.202595 \nL 246.292071 138.891642 \nL 251.416038 139.506828 \nL 256.540005 139.605343 \nL 261.663972 140.234834 \nL 266.787939 141.45881 \nL 271.911906 140.655473 \nL 277.035873 141.421629 \nL 282.15984 142.157898 \nL 287.283807 143.217375 \nL 292.407774 144.017639 \nL 297.531741 144.275276 \nL 302.655708 144.57909 \nL 307.779675 145.688362 \nL 312.903642 146.677079 \nL 318.027608 147.811954 \nL 323.151575 147.382757 \nL 328.275542 147.764745 \nL 333.399509 149.086456 \nL 338.523476 149.885557 \nL 343.647443 150.704154 \nL 348.77141 151.781848 \nL 353.895377 152.349973 \nL 359.019344 152.660439 \nL 364.143311 153.493484 \nL 369.267278 153.884066 \nL 374.391245 154.903067 \nL 379.515212 154.973913 \nL 384.639179 155.77214 \nL 389.763146 157.067587 \nL 394.887113 157.970936 \nL 400.01108 157.56566 \nL 405.135046 160.013428 \nL 410.259013 161.670408 \nL 415.38298 161.081493 \nL 420.506947 161.620149 \nL 425.630914 163.602286 \nL 430.754881 164.304997 \nL 435.878848 165.296047 \nL 441.002815 167.969853 \nL 446.126782 169.016879 \nL 451.250749 170.647313 \nL 456.374716 172.007963 \nL 461.498683 173.458257 \nL 466.62265 174.567691 \nL 471.746617 176.028196 \nL 476.870584 178.686221 \nL 481.994551 179.562831 \nL 487.118518 180.839208 \nL 492.242485 182.720633 \nL 497.366451 184.419222 \nL 502.490418 184.941702 \nL 507.614385 186.169981 \nL 512.738352 187.39882 \nL 517.862319 189.073818 \nL 522.986286 190.414938 \nL 528.110253 191.167099 \nL 533.23422 192.961428 \nL 538.358187 194.598879 \nL 543.482154 196.312477 \nL 548.606121 196.279068 \nL 553.730088 198.211171 \nL 558.854055 199.393856 \nL 563.978022 200.962176 \nL 569.101989 202.017049 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_28\">\n    <path clip-path=\"url(#p4e7628ff37)\" d=\"M 61.829261 266.645455 \nL 66.953228 259.604695 \nL 72.077195 259.420567 \nL 77.201162 258.848238 \nL 82.325129 258.361106 \nL 87.449096 258.264144 \nL 92.573063 257.549045 \nL 97.69703 257.06785 \nL 102.820997 256.552435 \nL 107.944964 255.977538 \nL 113.068931 255.068045 \nL 118.192898 254.504378 \nL 123.316865 252.993476 \nL 128.440832 252.439715 \nL 133.564799 251.746858 \nL 138.688765 250.996377 \nL 143.812732 250.338704 \nL 148.936699 249.890853 \nL 154.060666 249.278158 \nL 159.184633 248.778709 \nL 164.3086 248.131814 \nL 169.432567 247.727432 \nL 174.556534 247.690984 \nL 179.680501 247.090517 \nL 184.804468 246.773388 \nL 189.928435 245.902305 \nL 195.052402 246.106104 \nL 200.176369 245.491914 \nL 205.300336 245.604084 \nL 210.424303 244.940432 \nL 215.54827 244.773459 \nL 220.672237 244.347504 \nL 225.796204 244.366417 \nL 230.92017 244.021945 \nL 236.044137 243.912223 \nL 241.168104 243.560991 \nL 246.292071 242.890256 \nL 251.416038 242.59602 \nL 256.540005 242.529387 \nL 261.663972 242.307799 \nL 266.787939 241.71896 \nL 271.911906 242.009779 \nL 277.035873 241.645193 \nL 282.15984 241.287763 \nL 287.283807 241.152191 \nL 292.407774 240.526057 \nL 297.531741 240.748642 \nL 302.655708 240.443144 \nL 307.779675 240.014023 \nL 312.903642 239.690255 \nL 318.027608 239.384485 \nL 323.151575 239.464164 \nL 328.275542 238.847159 \nL 333.399509 238.686523 \nL 338.523476 238.632948 \nL 343.647443 238.103183 \nL 348.77141 237.700571 \nL 353.895377 237.57892 \nL 359.019344 237.289736 \nL 364.143311 236.916914 \nL 369.267278 236.980777 \nL 374.391245 236.549303 \nL 379.515212 236.392452 \nL 384.639179 236.310378 \nL 389.763146 235.679104 \nL 394.887113 235.506822 \nL 400.01108 235.602387 \nL 405.135046 234.569874 \nL 410.259013 234.108561 \nL 415.38298 234.059047 \nL 420.506947 234.006154 \nL 425.630914 233.595696 \nL 430.754881 233.138599 \nL 435.878848 232.712721 \nL 441.002815 231.746929 \nL 446.126782 231.135447 \nL 451.250749 231.00148 \nL 456.374716 230.542926 \nL 461.498683 229.755673 \nL 466.62265 228.932662 \nL 471.746617 228.778364 \nL 476.870584 227.891326 \nL 481.994551 227.102278 \nL 487.118518 227.04838 \nL 492.242485 225.910148 \nL 497.366451 225.642661 \nL 502.490418 225.331713 \nL 507.614385 224.692703 \nL 512.738352 224.056807 \nL 517.862319 223.744437 \nL 522.986286 223.088564 \nL 528.110253 222.482638 \nL 533.23422 222.173818 \nL 538.358187 221.064388 \nL 543.482154 221.159905 \nL 548.606121 220.902686 \nL 553.730088 220.025838 \nL 558.854055 219.720997 \nL 563.978022 219.191239 \nL 569.101989 218.645355 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_29\">\n    <path clip-path=\"url(#p4e7628ff37)\" d=\"M 61.829261 259.483624 \nL 66.953228 258.79578 \nL 72.077195 259.609597 \nL 77.201162 259.911897 \nL 82.325129 257.684061 \nL 87.449096 257.644832 \nL 92.573063 256.977682 \nL 97.69703 256.83512 \nL 102.820997 255.992976 \nL 107.944964 255.315816 \nL 113.068931 254.615873 \nL 118.192898 253.878477 \nL 123.316865 252.878343 \nL 128.440832 251.460873 \nL 133.564799 252.425429 \nL 138.688765 249.686597 \nL 143.812732 249.403787 \nL 148.936699 248.153627 \nL 154.060666 249.295588 \nL 159.184633 246.564218 \nL 164.3086 246.376257 \nL 169.432567 246.183288 \nL 174.556534 245.97471 \nL 179.680501 246.270571 \nL 184.804468 244.903502 \nL 189.928435 244.344736 \nL 195.052402 244.50143 \nL 200.176369 243.929794 \nL 205.300336 243.15753 \nL 210.424303 245.6341 \nL 215.54827 242.920032 \nL 220.672237 242.50148 \nL 225.796204 242.418536 \nL 230.92017 242.549977 \nL 236.044137 241.362588 \nL 241.168104 241.44792 \nL 246.292071 241.025321 \nL 251.416038 240.038748 \nL 256.540005 240.622614 \nL 261.663972 241.02044 \nL 266.787939 240.277036 \nL 271.911906 239.70074 \nL 277.035873 240.757078 \nL 282.15984 240.789266 \nL 287.283807 238.99595 \nL 292.407774 239.712295 \nL 297.531741 238.439128 \nL 302.655708 238.828845 \nL 307.779675 238.307767 \nL 312.903642 238.096367 \nL 318.027608 236.968155 \nL 323.151575 237.323049 \nL 328.275542 237.629879 \nL 333.399509 237.090332 \nL 338.523476 236.256301 \nL 343.647443 235.618828 \nL 348.77141 236.492174 \nL 353.895377 235.697042 \nL 359.019344 235.33279 \nL 364.143311 234.918217 \nL 369.267278 236.164569 \nL 374.391245 234.731547 \nL 379.515212 234.316349 \nL 384.639179 234.61538 \nL 389.763146 233.55625 \nL 394.887113 234.251742 \nL 400.01108 233.245632 \nL 405.135046 232.374756 \nL 410.259013 233.098143 \nL 415.38298 232.279064 \nL 420.506947 231.869767 \nL 425.630914 232.166555 \nL 430.754881 230.843022 \nL 435.878848 229.729282 \nL 441.002815 229.773014 \nL 446.126782 228.947264 \nL 451.250749 228.451449 \nL 456.374716 227.547439 \nL 461.498683 225.570605 \nL 466.62265 225.384071 \nL 471.746617 224.339675 \nL 476.870584 223.50893 \nL 481.994551 224.261854 \nL 487.118518 222.049578 \nL 492.242485 222.545409 \nL 497.366451 221.204975 \nL 502.490418 220.595427 \nL 507.614385 219.880863 \nL 512.738352 219.537092 \nL 517.862319 216.751101 \nL 522.986286 217.109949 \nL 528.110253 216.754384 \nL 533.23422 215.413239 \nL 538.358187 214.790315 \nL 543.482154 215.918329 \nL 548.606121 213.909719 \nL 553.730088 213.761779 \nL 558.854055 211.928457 \nL 563.978022 212.503419 \nL 569.101989 211.415488 \n\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 279 \nL 36.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 594.465625 279 \nL 594.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 279 \nL 594.465625 279 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 7.2 \nL 594.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 485.928125 59.234375 \nL 587.465625 59.234375 \nQ 589.465625 59.234375 589.465625 57.234375 \nL 589.465625 14.2 \nQ 589.465625 12.2 587.465625 12.2 \nL 485.928125 12.2 \nQ 483.928125 12.2 483.928125 14.2 \nL 483.928125 57.234375 \nQ 483.928125 59.234375 485.928125 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_30\">\n     <path d=\"M 487.928125 20.298437 \nL 507.928125 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_31\"/>\n    <g id=\"text_15\">\n     <!-- train loss -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(515.928125 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"292.333984\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"353.515625\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"405.615234\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_32\">\n     <path d=\"M 487.928125 34.976562 \nL 507.928125 34.976562 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_33\"/>\n    <g id=\"text_16\">\n     <!-- train acc -->\n     <g transform=\"translate(515.928125 38.476562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"325.830078\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"380.810547\" xlink:href=\"#DejaVuSans-99\"/>\n     </g>\n    </g>\n    <g id=\"line2d_34\">\n     <path d=\"M 487.928125 49.654687 \nL 507.928125 49.654687 \n\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_35\"/>\n    <g id=\"text_17\">\n     <!-- validation acc -->\n     <defs>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     </defs>\n     <g transform=\"translate(515.928125 53.154687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"176.025391\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"239.501953\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"300.78125\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"339.990234\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"367.773438\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"428.955078\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"492.333984\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"524.121094\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"585.400391\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"640.380859\" xlink:href=\"#DejaVuSans-99\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4e7628ff37\">\n   <rect height=\"271.8\" width=\"558\" x=\"36.465625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"},"metadata":{"needs_background":"light"}}],"source":["net = TransformerNetwork()\n","train(net, data_loaders, epochs=100, lr=0.001)"]},{"cell_type":"markdown","metadata":{"id":"5dXD1qzHnnhT"},"source":["**(c) Briefly discuss the results. Has the training converged? Is this a good calculator?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"SSh0y62BnnhT"},"source":["The results are fair, but the training has not converged yet. The loss seems to be decreasing even when being close to epoch nº100, while the oppossite appears to happen with the accuracy. We do not think this calculator is good because it could be improved further."]},{"cell_type":"markdown","metadata":{"id":"m9EpkPlQnnhT"},"source":["**(d) Run the trained network with input `\"123+123\"` and `\"321+321\"`.<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0KUi8pORnnhT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e4b50439-1be8-43ac-f306-af9093305bad"},"outputs":[{"output_type":"stream","name":"stdout","text":["[['3' '2' '5']\n"," ['6' '5' '2']]\n"]}],"source":["def predict(net, q, a):\n","    # Run net to predict the output given the input q=a.\n","    with torch.no_grad():\n","        bos = torch.tensor(vocab['<bos>']).expand(a.shape[0], 1)\n","        a_prev = torch.cat((bos, a[:, :-1]), axis=1)\n","        y_hat = net(q, a_prev)\n","        return y_hat\n","\n","padded_q = []\n","padded_a = []\n","\n","for q, a in [('123+123', '246'), ('321+321', '642')]:\n","  tokens_q = tokenize_and_encode(q, vocab)\n","  tokens_a = tokenize_and_encode(a, vocab)\n","  padded_q.append(pad_or_trim(tokens_q, 7, vocab))\n","  padded_a.append(pad_or_trim(tokens_a, 3, vocab))\n","res = predict(net, torch.tensor(padded_q), torch.tensor(padded_a))\n","print(decode_tokens(torch.argmax(res, dim=2)))\n"]},{"cell_type":"markdown","metadata":{"id":"FAC1Qe5FnnhT"},"source":["**(e) Compare the predictions for the first element of y. Can you explain what happens?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"lwOQ8z-VnnhT"},"source":["The predictions are awful and do not match the actual results"]},{"cell_type":"markdown","metadata":{"id":"XMMR3jbSnnhT"},"source":["**(f) Does the validation accuracy indicate how often the model is able to answers unseen formula correctly? Explain your answer.<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"AVEe8F1tnnhT"},"source":["The validation accuracy is not so bad, albeit the results for our single examples were very far from the actual ones. This shows that the evaluation of unseen formulas is very likely to fail."]},{"cell_type":"markdown","metadata":{"id":"K9ALRnzMnnhT"},"source":["**(g) If the forward function takes the shifted output `y_prev` as input, how can we use it if we don't know the output yet?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"_4DsNBANnnhU"},"source":["y_prev is neccesary because we need a masked target vector to feed the outputs as they are being produced by the network. Since each output token is generated one by one, it could be feeded as they are being retrieved."]},{"cell_type":"markdown","metadata":{"id":"4wqHSRmKnnhU"},"source":["## 5.5 Positional encoding (5 points)\n","\n","We did not yet include positional encoding in the network.\n","PyTorch does not include such an encoder, so here we copied the code from the book (slightly modified):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xLIteqpnnhU"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    \"\"\"Positional encoding.\"\"\"\n","    def __init__(self, num_hiddens, max_len=1000):\n","        super().__init__()\n","        # Create a long enough P\n","        self.P = torch.zeros((1, max_len, num_hiddens))\n","        X = torch.arange(max_len, dtype=torch.float32).reshape(\n","            -1, 1) / torch.pow(10000, torch.arange(\n","            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n","        self.P[:, :, 0::2] = torch.sin(X)\n","        self.P[:, :, 1::2] = torch.cos(X)\n","\n","    def forward(self, X):\n","        return X + self.P[:, :X.shape[1], :].to(X.device)"]},{"cell_type":"markdown","metadata":{"id":"-BHxI-oWnnhU"},"source":["**(a) Add positional encoding to the TransformerModel.<span style=\"float:right\"> (point given in earlier question)</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kd0BZJ0tnnhU"},"outputs":[],"source":["class TransformerNetwork(torch.nn.Module):\n","    def __init__(self,\n","                 dim_vocab=len(vocab), padding_token=vocab['<pad>'],\n","                 num_layers=2, num_heads=4, dim_hidden=64, dim_feedforward=64,\n","                 dropout=0.01, positional_encoding=True):\n","        super().__init__()\n","        self.padding_token = padding_token\n","\n","        self.embedding    = torch.nn.Embedding(dim_vocab, dim_hidden)\n","        self.dropout      = torch.nn.Dropout()\n","        self.transformer  = torch.nn.Transformer(dim_hidden, num_heads, num_layers, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n","        self.predict      = torch.nn.Linear(dim_hidden, dim_vocab)\n","        if positional_encoding:\n","            self.pos_encoding = PositionalEncoding(dim_hidden)\n","        else:\n","            self.pos_encoding = torch.nn.Identity()\n","\n","    def forward(self, src, tgt):\n","        embeded_src = self.pos_encoding(self.dropout(self.embedding(src)))\n","        embeded_tgt = self.pos_encoding(self.dropout(self.embedding(tgt)))\n","        src_mask = generate_square_subsequent_mask(src.shape[1])\n","        tgt_mask = generate_square_subsequent_mask(tgt.shape[1])\n","        src_padding_mask = generate_padding_mask(src, self.padding_token)\n","        tgt_padding_mask = generate_padding_mask(tgt, self.padding_token)\n","        result = self.transformer(embeded_src, embeded_tgt, src_mask, tgt_mask, src_key_padding_mask=src_padding_mask, tgt_key_padding_mask=tgt_padding_mask)\n","        return self.predict(result)"]},{"cell_type":"markdown","metadata":{"id":"V6oW-yJPnnhU"},"source":["**(b) Construct and train a network with positional encoding<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgnTW_GannhU","colab":{"base_uri":"https://localhost:8080/","height":478},"outputId":"bef3175e-89ae-49fd-f198-32841e56b632"},"outputs":[{"output_type":"stream","name":"stdout","text":["train loss 0.746, train acc 0.716, val acc 0.706\n","1202.1 examples/sec on cpu\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 720x360 with 1 Axes>"],"image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"316.55625pt\" version=\"1.1\" viewBox=\"0 0 601.665625 316.55625\" width=\"601.665625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 316.55625 \nL 601.665625 316.55625 \nL 601.665625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 279 \nL 594.465625 279 \nL 594.465625 7.2 \nL 36.465625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 56.705294 279 \nL 56.705294 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m04eee0c640\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.705294\" xlink:href=\"#m04eee0c640\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(53.524044 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 159.184633 279 \nL 159.184633 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"159.184633\" xlink:href=\"#m04eee0c640\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(152.822133 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 261.663972 279 \nL 261.663972 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"261.663972\" xlink:href=\"#m04eee0c640\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(255.301472 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 364.143311 279 \nL 364.143311 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"364.143311\" xlink:href=\"#m04eee0c640\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(357.780811 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 466.62265 279 \nL 466.62265 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"466.62265\" xlink:href=\"#m04eee0c640\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(460.26015 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 569.101989 279 \nL 569.101989 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"569.101989\" xlink:href=\"#m04eee0c640\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(559.558239 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     </defs>\n     <g transform=\"translate(300.2375 307.276562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 36.465625 277.657228 \nL 594.465625 277.657228 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5734574ca9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5734574ca9\" y=\"277.657228\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.25 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(7.2 281.456447)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 36.465625 242.878074 \nL 594.465625 242.878074 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5734574ca9\" y=\"242.878074\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.50 -->\n      <g transform=\"translate(7.2 246.677292)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 36.465625 208.098919 \nL 594.465625 208.098919 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5734574ca9\" y=\"208.098919\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.75 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(7.2 211.898138)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 36.465625 173.319764 \nL 594.465625 173.319764 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5734574ca9\" y=\"173.319764\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.00 -->\n      <g transform=\"translate(7.2 177.118983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 36.465625 138.54061 \nL 594.465625 138.54061 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5734574ca9\" y=\"138.54061\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.25 -->\n      <g transform=\"translate(7.2 142.339828)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 36.465625 103.761455 \nL 594.465625 103.761455 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5734574ca9\" y=\"103.761455\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.50 -->\n      <g transform=\"translate(7.2 107.560674)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 36.465625 68.9823 \nL 594.465625 68.9823 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5734574ca9\" y=\"68.9823\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.75 -->\n      <g transform=\"translate(7.2 72.781519)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 36.465625 34.203146 \nL 594.465625 34.203146 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m5734574ca9\" y=\"34.203146\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 2.00 -->\n      <g transform=\"translate(7.2 38.002364)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_29\">\n    <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 61.829261 19.554545 \nL 66.953228 71.828021 \nL 72.077195 82.226004 \nL 77.201162 85.519796 \nL 82.325129 91.112121 \nL 87.449096 94.661967 \nL 92.573063 98.439068 \nL 97.69703 103.607388 \nL 102.820997 108.21101 \nL 107.944964 113.20782 \nL 113.068931 117.514109 \nL 118.192898 121.394144 \nL 123.316865 124.239679 \nL 128.440832 127.013677 \nL 133.564799 129.975803 \nL 138.688765 129.953244 \nL 143.812732 133.811771 \nL 148.936699 135.83234 \nL 154.060666 137.043363 \nL 159.184633 138.411484 \nL 164.3086 138.488856 \nL 169.432567 140.402066 \nL 174.556534 140.450939 \nL 179.680501 142.460221 \nL 184.804468 142.88946 \nL 189.928435 144.13551 \nL 195.052402 145.149485 \nL 200.176369 145.353357 \nL 205.300336 146.629829 \nL 210.424303 147.018143 \nL 215.54827 147.12195 \nL 220.672237 148.072932 \nL 225.796204 146.983684 \nL 230.92017 149.42834 \nL 236.044137 150.264365 \nL 241.168104 150.345472 \nL 246.292071 151.081538 \nL 251.416038 152.131687 \nL 256.540005 152.701251 \nL 261.663972 152.410441 \nL 266.787939 153.390987 \nL 271.911906 153.905602 \nL 277.035873 153.086614 \nL 282.15984 154.330179 \nL 287.283807 154.626956 \nL 292.407774 155.831159 \nL 297.531741 156.236152 \nL 302.655708 155.668815 \nL 307.779675 156.011962 \nL 312.903642 156.27923 \nL 318.027608 156.78607 \nL 323.151575 158.761852 \nL 328.275542 158.637042 \nL 333.399509 158.639194 \nL 338.523476 158.043193 \nL 343.647443 159.538103 \nL 348.77141 160.55437 \nL 353.895377 160.785683 \nL 359.019344 161.877913 \nL 364.143311 161.975245 \nL 369.267278 162.053247 \nL 374.391245 162.456345 \nL 379.515212 163.10768 \nL 384.639179 164.335697 \nL 389.763146 164.594131 \nL 394.887113 164.20718 \nL 400.01108 165.455085 \nL 405.135046 166.392886 \nL 410.259013 166.001162 \nL 415.38298 167.79363 \nL 420.506947 169.087328 \nL 425.630914 170.406539 \nL 430.754881 172.912869 \nL 435.878848 169.870372 \nL 441.002815 175.713777 \nL 446.126782 178.946252 \nL 451.250749 181.125882 \nL 456.374716 184.186581 \nL 461.498683 184.544286 \nL 466.62265 186.869535 \nL 471.746617 186.810303 \nL 476.870584 190.112425 \nL 481.994551 191.850257 \nL 487.118518 194.363864 \nL 492.242485 195.368125 \nL 497.366451 197.27892 \nL 502.490418 197.801746 \nL 507.614385 200.410886 \nL 512.738352 201.42641 \nL 517.862319 201.664556 \nL 522.986286 203.384158 \nL 528.110253 202.78008 \nL 533.23422 204.939729 \nL 538.358187 205.708175 \nL 543.482154 206.192752 \nL 548.606121 207.024288 \nL 553.730088 208.604074 \nL 558.854055 208.644984 \nL 563.978022 208.955634 \nL 569.101989 208.649156 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 61.829261 266.645455 \nL 66.953228 256.325533 \nL 72.077195 255.7099 \nL 77.201162 255.427269 \nL 82.325129 254.584255 \nL 87.449096 254.061998 \nL 92.573063 252.805981 \nL 97.69703 251.201022 \nL 102.820997 249.920023 \nL 107.944964 248.776053 \nL 113.068931 247.04052 \nL 118.192898 245.873721 \nL 123.316865 244.995112 \nL 128.440832 243.400122 \nL 133.564799 243.255949 \nL 138.688765 242.46664 \nL 143.812732 241.542384 \nL 148.936699 240.685471 \nL 154.060666 239.973788 \nL 159.184633 239.802378 \nL 164.3086 239.529048 \nL 169.432567 238.927519 \nL 174.556534 239.115983 \nL 179.680501 237.95132 \nL 184.804468 237.843746 \nL 189.928435 237.067214 \nL 195.052402 236.813809 \nL 200.176369 236.865493 \nL 205.300336 236.081353 \nL 210.424303 236.244247 \nL 215.54827 236.427049 \nL 220.672237 235.763927 \nL 225.796204 236.131431 \nL 230.92017 235.270397 \nL 236.044137 234.885446 \nL 241.168104 234.777819 \nL 246.292071 234.599877 \nL 251.416038 234.001188 \nL 256.540005 233.831606 \nL 261.663972 233.820978 \nL 266.787939 233.621188 \nL 271.911906 233.228418 \nL 277.035873 233.887581 \nL 282.15984 233.06501 \nL 287.283807 233.100292 \nL 292.407774 232.543054 \nL 297.531741 232.422659 \nL 302.655708 232.484924 \nL 307.779675 232.347745 \nL 312.903642 231.923567 \nL 318.027608 232.070376 \nL 323.151575 231.291653 \nL 328.275542 231.109922 \nL 333.399509 231.394727 \nL 338.523476 231.458638 \nL 343.647443 231.054845 \nL 348.77141 230.471921 \nL 353.895377 230.419384 \nL 359.019344 230.015276 \nL 364.143311 229.851691 \nL 369.267278 230.016067 \nL 374.391245 229.607953 \nL 379.515212 229.595841 \nL 384.639179 229.204727 \nL 389.763146 228.791012 \nL 394.887113 229.058875 \nL 400.01108 228.56566 \nL 405.135046 228.368194 \nL 410.259013 228.52268 \nL 415.38298 227.819402 \nL 420.506947 227.358123 \nL 425.630914 227.031259 \nL 430.754881 226.20993 \nL 435.878848 226.837972 \nL 441.002815 225.157043 \nL 446.126782 224.460582 \nL 451.250749 223.694376 \nL 456.374716 222.717432 \nL 461.498683 222.242812 \nL 466.62265 221.822563 \nL 471.746617 221.586248 \nL 476.870584 220.14823 \nL 481.994551 219.679901 \nL 487.118518 218.732294 \nL 492.242485 218.561032 \nL 497.366451 217.692 \nL 502.490418 217.430675 \nL 507.614385 216.282657 \nL 512.738352 215.876131 \nL 517.862319 215.600463 \nL 522.986286 215.096802 \nL 528.110253 215.217485 \nL 533.23422 214.365375 \nL 538.358187 214.37159 \nL 543.482154 213.880311 \nL 548.606121 213.557459 \nL 553.730088 212.932454 \nL 558.854055 212.950237 \nL 563.978022 212.66589 \nL 569.101989 212.862495 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path clip-path=\"url(#p6fe6b80c0a)\" d=\"M 61.829261 255.269737 \nL 66.953228 255.465231 \nL 72.077195 256.137107 \nL 77.201162 254.751907 \nL 82.325129 253.128372 \nL 87.449096 252.289469 \nL 92.573063 251.028093 \nL 97.69703 249.973552 \nL 102.820997 248.275052 \nL 107.944964 247.277843 \nL 113.068931 244.646586 \nL 118.192898 245.400593 \nL 123.316865 243.041227 \nL 128.440832 242.585333 \nL 133.564799 241.041624 \nL 138.688765 240.248185 \nL 143.812732 240.140019 \nL 148.936699 239.019878 \nL 154.060666 238.547531 \nL 159.184633 238.2649 \nL 164.3086 237.962306 \nL 169.432567 237.265658 \nL 174.556534 237.819804 \nL 179.680501 237.969902 \nL 184.804468 236.760862 \nL 189.928435 235.982108 \nL 195.052402 236.349182 \nL 200.176369 235.766234 \nL 205.300336 235.389913 \nL 210.424303 235.912135 \nL 215.54827 235.080551 \nL 220.672237 235.5455 \nL 225.796204 235.060857 \nL 230.92017 234.339837 \nL 236.044137 235.015799 \nL 241.168104 235.193385 \nL 246.292071 235.11884 \nL 251.416038 234.667182 \nL 256.540005 234.237955 \nL 261.663972 234.37448 \nL 266.787939 233.604038 \nL 271.911906 233.772687 \nL 277.035873 234.275856 \nL 282.15984 234.287465 \nL 287.283807 234.097092 \nL 292.407774 233.130922 \nL 297.531741 233.478756 \nL 302.655708 233.363762 \nL 307.779675 233.008408 \nL 312.903642 233.665385 \nL 318.027608 232.687018 \nL 323.151575 232.912516 \nL 328.275542 233.166286 \nL 333.399509 232.809979 \nL 338.523476 233.214693 \nL 343.647443 232.366444 \nL 348.77141 231.964206 \nL 353.895377 232.026417 \nL 359.019344 231.988464 \nL 364.143311 232.117156 \nL 369.267278 231.923351 \nL 374.391245 231.283134 \nL 379.515212 231.531962 \nL 384.639179 231.180698 \nL 389.763146 231.650724 \nL 394.887113 230.912866 \nL 400.01108 230.442869 \nL 405.135046 230.357446 \nL 410.259013 229.536292 \nL 415.38298 229.895353 \nL 420.506947 229.557065 \nL 425.630914 229.286344 \nL 430.754881 227.931685 \nL 435.878848 227.437049 \nL 441.002815 226.230479 \nL 446.126782 225.600389 \nL 451.250749 224.23813 \nL 456.374716 224.117253 \nL 461.498683 224.238041 \nL 466.62265 222.446761 \nL 471.746617 221.639395 \nL 476.870584 220.885364 \nL 481.994551 220.30554 \nL 487.118518 218.364413 \nL 492.242485 218.898394 \nL 497.366451 217.818951 \nL 502.490418 217.163965 \nL 507.614385 216.615878 \nL 512.738352 215.79725 \nL 517.862319 215.988005 \nL 522.986286 216.555218 \nL 528.110253 216.075065 \nL 533.23422 214.393604 \nL 538.358187 214.215074 \nL 543.482154 214.660513 \nL 548.606121 213.750789 \nL 553.730088 214.053901 \nL 558.854055 214.130694 \nL 563.978022 213.153628 \nL 569.101989 214.161252 \n\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 279 \nL 36.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 594.465625 279 \nL 594.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 279 \nL 594.465625 279 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 7.2 \nL 594.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 485.928125 59.234375 \nL 587.465625 59.234375 \nQ 589.465625 59.234375 589.465625 57.234375 \nL 589.465625 14.2 \nQ 589.465625 12.2 587.465625 12.2 \nL 485.928125 12.2 \nQ 483.928125 12.2 483.928125 14.2 \nL 483.928125 57.234375 \nQ 483.928125 59.234375 485.928125 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_32\">\n     <path d=\"M 487.928125 20.298437 \nL 507.928125 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_33\"/>\n    <g id=\"text_16\">\n     <!-- train loss -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(515.928125 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"292.333984\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"353.515625\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"405.615234\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_34\">\n     <path d=\"M 487.928125 34.976562 \nL 507.928125 34.976562 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_35\"/>\n    <g id=\"text_17\">\n     <!-- train acc -->\n     <g transform=\"translate(515.928125 38.476562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"325.830078\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"380.810547\" xlink:href=\"#DejaVuSans-99\"/>\n     </g>\n    </g>\n    <g id=\"line2d_36\">\n     <path d=\"M 487.928125 49.654687 \nL 507.928125 49.654687 \n\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_37\"/>\n    <g id=\"text_18\">\n     <!-- validation acc -->\n     <defs>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     </defs>\n     <g transform=\"translate(515.928125 53.154687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"176.025391\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"239.501953\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"300.78125\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"339.990234\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"367.773438\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"428.955078\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"492.333984\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"524.121094\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"585.400391\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"640.380859\" xlink:href=\"#DejaVuSans-99\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6fe6b80c0a\">\n   <rect height=\"271.8\" width=\"558\" x=\"36.465625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"},"metadata":{"needs_background":"light"}}],"source":["net_pos = TransformerNetwork()\n","train(net_pos, data_loaders, epochs=100, lr=0.001)"]},{"cell_type":"markdown","metadata":{"id":"NfRvmVAWnnhU"},"source":["**(c) How does the performance of a model with positional encoding compare to a model without?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"-p-6M7VDnnhU"},"source":["We can see that positial encoding improves the results of our Transformer Network. Both accuracies and loss are considerably improved. That is because If we don't use positional encoding the model doesn't have a way to take the sequence of the input into consideration. This is because the output is computed independently and therefore does not model the sequence. It would mean that the sequence of a sentence is not important."]},{"cell_type":"markdown","metadata":{"id":"0CAQLMfQnnhU"},"source":["**(d) Run the trained network with input `\"123+123\"` and `\"321+321\"`.<span style=\"float:right\"> (no points)</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CaRAaAtinnhV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6ab7e9bb-25b4-46f7-b446-db2bb95fa795"},"outputs":[{"output_type":"stream","name":"stdout","text":["[['2' '3' '7' '<eos>']\n"," ['6' '3' '2' '<eos>']]\n"]}],"source":["def predict(net, q, a):\n","    # Run net to predict the output given the input q=a.\n","    with torch.no_grad():\n","        bos = torch.tensor(vocab['<bos>']).expand(a.shape[0], 1)\n","        a_prev = torch.cat((bos, a[:, :-1]), axis=1)\n","        y_hat = net(q,a_prev)\n","        return y_hat\n","\n","padded_q = []\n","padded_a = []\n","\n","for q, a in [('123+123', '246'), ('321+321', '642')]:\n","  tokens_q = tokenize_and_encode(q, vocab)\n","  tokens_a = tokenize_and_encode(a, vocab)\n","  padded_q.append(pad_or_trim(tokens_q, 7, vocab))\n","  padded_a.append(pad_or_trim(tokens_a, 4, vocab))\n","res = predict(net_pos, torch.tensor(padded_q), torch.tensor(padded_a))\n","print(decode_tokens(torch.argmax(res, dim=2)))"]},{"cell_type":"markdown","metadata":{"id":"d7HIpDfWnnhV"},"source":["**(e) Compare the predictions for the first element of y with what you found earlier. Can you explain what happens?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"2ACTZ3cQnnhV"},"source":["The first element is now perfectly predicted and the rest ones are pretty close to the correct ones."]},{"cell_type":"markdown","metadata":{"id":"QCelO4PmnnhV"},"source":["**(f) Explain in your own words why positional encoding is used in transformer networks.<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"WG3pGFhYnnhV"},"source":["As we said in previous question, without positional encoding  the output is computed independently and therefore does not model the sequence. The positional encoding takes into account the sequence of a sentence."]},{"cell_type":"markdown","metadata":{"id":"FIVyQDI5nnhV"},"source":["**(g) Look at the learning curve. Can you suggest a way to improve the model?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"oUj7M2VxnnhV"},"source":["Since the learning increased around the last epochs, this means that we could have increased the lerning rate to achieve a more rapid convergence. Or we could have used more epochs instead."]},{"cell_type":"markdown","metadata":{"id":"5S7AkUAknnhV"},"source":["**(h) Optional: if time permits, try to train an even better model**"]},{"cell_type":"markdown","metadata":{"id":"8byHykrdnnhV"},"source":["## 5.6 Predicting for new samples (5 points)"]},{"cell_type":"markdown","metadata":{"id":"p0eOsmJznnhV"},"source":["Predicting an output given a new sample requires an appropriate search algorithm (see [d2l chapter 10.8](https://d2l.ai/chapter_recurrent-modern/beam-search.html)). Here, we will implement the simplest form: a greedy search algorithm that selects the token with the highest probability at each time step.\n","\n","**(a) Describe this search strategy in pseudo-code.<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"9Vul5vuannhW"},"source":["```\n","output = []\n","input = [ ( ... input tokens ...) ]\n","tokens = [ ( ... possible tokens ... ) ]\n","for i in (0 to input.size)\n","  max_token_probability_index = probabilities.indexOf(max(probabilities[i]))\n","  output.append( tokens[max_token_probability_index] )\n","return outoput;\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qcVegQQQnnhW"},"source":["**(b) Implement a greedy search function to predict a sequence using `net_pos`.<span style=\"float:right\"> (2 points)</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IoqmIJXennhW"},"outputs":[],"source":["def predict_greedy(net, src, length):\n","    # predict an output sequence of the given (maximum) length given input string src\n","    with torch.no_grad():\n","        tokens_q = tokenize_and_encode(src, vocab)\n","        padded_q = torch.tensor([pad_or_trim(tokens_q, length, vocab)])\n","        matrix = []\n","        for v in vocab.idx_to_token:\n","          preds = []\n","          v_t = torch.tensor([[vocab[v]]])\n","          for time_step in range(0,length+1):\n","            y_hat = net(padded_q,v_t)\n","            preds.append(y_hat)\n","          matrix.append(preds)\n","        i = 0\n","        output = []\n","        for token in src:\n","          output.append(matrix[vocab[token]][i])\n","          i += 1\n","        return output\n","\n","\n","predicted_sequence = predict_greedy(net_pos, '123+123', 6)\n","#print(predicted_sequence)\n","print([t.numpy() for t in decode_tokens(predicted_sequence)])"]},{"cell_type":"markdown","metadata":{"id":"FXFkqMnwnnhW"},"source":["**(c) Does this search strategy give a high-quality prediction? Why, or why not?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"5myNKdj-nnhW"},"source":["This search strategy give us a a good quality prediction, is not a high-quality prediction because this strategy aims mainly at reducing the computational cost, so the choice is made on the basis of the token with the highest conditional probability."]},{"cell_type":"markdown","metadata":{"id":"9_FO-5vZnnhW"},"source":["**(d) What alternative search strategy could we use to improve the predictions? Why would this help?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"cPsXZn0HnnhW"},"source":["If the goal is to obtain the most likely sequence, we may consider using exhaustive search: exhaustively enumerate all the possible output sequences with their conditional probabilities, and then output the one that scores the highest predicted probability.The problem os this kind of searc is the computational cost. On another hand, we have the Beam search strategy, this one is attempt to provide a tradeoff between accuracy versus computational cost via its flexible choice of the beam size."]},{"cell_type":"markdown","metadata":{"id":"j_SQJxdwnnhW"},"source":["## 5.7 Discussion (5 points)\n","\n","Last week, we looked at recurrent neural networks such as the LSTM. Both recurrent neural networks and transformers work with sequences, but in recent years the transformer has become more popular than the recurrent models.\n","\n","**(a) What is an important advantage of transformers over recurrent neural networks?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"Lmk5MkJCnnhX"},"source":["The main advantage is related to the possibility of paying attention to specific parts of the input. Instead of just operating with them as if they were independent tokens with little relation."]},{"cell_type":"markdown","metadata":{"id":"5I8yyulXnnhX"},"source":["**(b) Does this advantage also hold when predicting outputs for new sequences? Why, or why not?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"ylKEOtnbnnhX"},"source":["If the model is well trained and parametrized, i shopuld be capable of using attention when processing other inputs as well."]},{"cell_type":"markdown","metadata":{"id":"XLpQUTTdnnhX"},"source":["**(c) Why is positional encoding often used in transformers, but not in convolutional or recurrent neural networks?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"foSm7C5-nnhY"},"source":["Because the other neural networks have no sense of the position of each token alongside the input (RNNs might not even have the whole input at hand). So the positional encoding allows to observe with different attention to each specifi token conforming the input.\n"]},{"cell_type":"markdown","metadata":{"id":"x7f11wLNnnhY"},"source":["The structure of a recurrent neural network makes it very suitable for online predictions, such as real-time translation, because it only depends on prior inputs. (Note: 'online' means giving predictions immediately, as opposed to collecting a full dataset and analyzing it afterwards, it has nothing to do with the internet)\n","\n","**(d) How would a transformer work in an online application? Do you need to change the architecture?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"MmdpUlLnnnhY"},"source":["Since the transformer needs to process the whole input all at once, we could train the network to handle each separate token as an input, computing the online output one by one\n","Also, it would be possible to stack many transformers feeding their outputs as inputs to others, so that the network architecture allows handling many inputs at once."]},{"cell_type":"markdown","metadata":{"id":"J6EjS6pLnnhY"},"source":["**(e) How would you train a transformer for an online task?<span style=\"float:right\"> (1 point)</span>**"]},{"cell_type":"markdown","metadata":{"id":"AJCgb5I7nnhY"},"source":["The input would be given one by one to the first transformer and the output would be later retrieved from each of the layers.\n","The output size could be as big as the number of layers in the architecture.  "]},{"cell_type":"markdown","metadata":{"id":"NQ7GrZjEnnhY"},"source":["## The end\n","\n","Well done! Please double check the instructions at the top before you submit your results."]},{"cell_type":"markdown","metadata":{"id":"J2a40TIHnnhZ"},"source":["*This assignment has 47 points.*\n","<span style=\"float:right;color:#aaa;font-size:10px;\"> Version 542f88c / 2022-10-05</span>"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"toc-showmarkdowntxt":false,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}